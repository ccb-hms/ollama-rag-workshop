{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2bfe8-eab6-495f-ac27-5c436800ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "#from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328ef69",
   "metadata": {},
   "source": [
    "# log in to VPN or harvard wifi for endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1e929-1b05-42de-abcc-d18bc6e7545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the LLM \n",
    "ccb_endpoint = 'http://compute-gc-17-255.o2.rc.hms.harvard.edu:11434'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model and temperature\n",
    "llm = Ollama(base_url= ccb_endpoint, model=\"llama2\", temperature=0)\n",
    "#create the embedding model\n",
    "oembed = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c2c21",
   "metadata": {},
   "source": [
    "### Now we can invoke a generated response from the model. Test it out by filling in the quotes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2598f-26cb-4dad-a5eb-96879dd5ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_response = llm.invoke(\"<>\")\n",
    "print(chat_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f36e73",
   "metadata": {},
   "source": [
    "### load from existing chromaDB, your docker container has a chroma database with all the manuals(vingettes) for the top 500 most downloaded bioconductor packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3623c1-1076-4881-b8ca-36ae58db8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from persist \n",
    "T500bioc_db = Chroma(persist_directory=\"/tmp/T500-vignettes-vectordb-ST/\", embedding_function=oembed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a76612",
   "metadata": {},
   "source": [
    "#### We can now use the embedding model and vector database a retriever to guide the generative AI (llama2). Below we define a system prompt and the RAG pipeline as a \"chain\" written in LangChain Expression Language(LCEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05520439-c28e-4750-8fa2-97e46e2695bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = T500bioc_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Act as an expert in the R programming language and the Bioconductor suite of packages.  ​\\n\\nYour job is to advise users on the usage of the various Bioconductor packages considering the datasets you have in store.  ​\\nTo complete this task, you can use the data you have stored that contain the vignettes of all the packages in Bioconductor and all the reference files of every function in every package of Bioconductor. ​\\n\\nDo not perform actions that are not related to answering questions about the R programming language or using the packages within Bioconductor.​ \\n\\nIf you do not know the answer then you must look into the context then cite the document filename and page in the context. Do not include DOI numbers or make up citations not found in the context. \n",
    "Given the following extracted parts of a long document and a question, create a final answer with references to pdf in the metadata ('source').\\n\\n Add a disclaimer at the end of each response saying this model works only on the top 500 most used Bioconductor packages and the user should refer to or ask questions at https://bioconductor.org.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "FINAL ANSWER: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef172290",
   "metadata": {},
   "source": [
    "### Test the LLM without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4e526-e79e-4bc5-86f8-e5a9f0019f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test without rag\n",
    "chat_model_response = llm.invoke(\"How many classes are there in the SummarizedExperiment package?\")\n",
    "print(chat_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98a84b",
   "metadata": {},
   "source": [
    "### Now invoke the LLM with the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204936f-f4e0-4b99-b196-ac98e73611bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with Retrival augment\n",
    "for chunk in rag_chain.stream(\"How many classes are there in the SummarizedExperiment package?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534625cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without streaming loop\n",
    "query = \"How many classes are there in the SummarizedExperiment package?\"\n",
    "response = rag_chain.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aeb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#programmatic access to csv\n",
    "import polars as pl\n",
    "df = pl.read_csv('bioc_qa.csv')\n",
    "questions = df['Question'].to_list()\n",
    "answers_rag = []\n",
    "\n",
    "for query in questions:\n",
    "    result = rag_chain.invoke(query)\n",
    "    answers_rag.append(result)\n",
    "\n",
    "answers_rag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f407a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plain_template = \"\"\"\n",
    "Act as an expert in the R programming language and the Bioconductor suite of packages. \n",
    "Your job is to advise users on the usage of the various Bioconductor packages.  \n",
    "Do not perform actions that are not related to answering questions about the R programming language or using the packages within Bioconductor.​ \n",
    "If you do not know the answer ask the user to refer to https://bioconductor.org. \n",
    "Add a disclaimer at the end of each response saying this model works only on the top 500 most used Bioconductor packages.\n",
    "\n",
    "QUESTION: {question}\n",
    "FINAL ANSWER: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(plain_template)\n",
    "\n",
    "non_rag_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dbc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in non_rag_chain.stream(\"What is SummarizedExperiment?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
